{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "65h8p7iuMsqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "b7f78575-0bbe-40c5-ec66-a52d914f9fdf"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgPWiYMHvOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "948bcef2-80af-4f94-fccf-7cbafe3ccfc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtQ7Y-itNCni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "681816fd-82df-468f-9807-7aa35b6266b9"
      },
      "source": [
        "import matplotlib\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOMTjCi-NR0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    df = pd.read_csv('/content/drive/My Drive/combined-swell-classification-eda-dataset.csv')\n",
        "except:\n",
        "    print(\"\"\"\n",
        "      Dataset not found in your computer.\n",
        "      \"\"\")\n",
        "    quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aChkqnLQNU3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c09a341e-2751-401d-f40c-7bd7ce2d7b9c"
      },
      "source": [
        "X = df.drop(columns=['condition','NasaTLX class','Condition Label','NasaTLX Label'],axis=1)\n",
        "X[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MEAN</th>\n",
              "      <th>MAX</th>\n",
              "      <th>MIN</th>\n",
              "      <th>RANGE</th>\n",
              "      <th>KURT</th>\n",
              "      <th>SKEW</th>\n",
              "      <th>MEAN_1ST_GRAD</th>\n",
              "      <th>STD_1ST_GRAD</th>\n",
              "      <th>MEAN_2ND_GRAD</th>\n",
              "      <th>STD_2ND_GRAD</th>\n",
              "      <th>ALSC</th>\n",
              "      <th>INSC</th>\n",
              "      <th>APSC</th>\n",
              "      <th>RMSC</th>\n",
              "      <th>MIN_PEAKS</th>\n",
              "      <th>MAX_PEAKS</th>\n",
              "      <th>STD_PEAKS</th>\n",
              "      <th>MEAN_PEAKS</th>\n",
              "      <th>MIN_ONSET</th>\n",
              "      <th>MAX_ONSET</th>\n",
              "      <th>STD_ONSET</th>\n",
              "      <th>MEAN_ONSET</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>MEAN_LOG</th>\n",
              "      <th>INSC_LOG</th>\n",
              "      <th>APSC_LOG</th>\n",
              "      <th>RMSC_LOG</th>\n",
              "      <th>RANGE_LOG</th>\n",
              "      <th>ALSC_LOG</th>\n",
              "      <th>MIN_LOG</th>\n",
              "      <th>MEAN_1ST_GRAD_LOG</th>\n",
              "      <th>MEAN_2ND_GRAD_LOG</th>\n",
              "      <th>MIN_LOG_LOG</th>\n",
              "      <th>MEAN_1ST_GRAD_LOG_LOG</th>\n",
              "      <th>MEAN_2ND_GRAD_LOG_LOG</th>\n",
              "      <th>APSC_LOG_LOG</th>\n",
              "      <th>ALSC_LOG_LOG</th>\n",
              "      <th>APSC_BOXCOX</th>\n",
              "      <th>RMSC_BOXCOX</th>\n",
              "      <th>RANGE_BOXCOX</th>\n",
              "      <th>MEAN_YEO_JONSON</th>\n",
              "      <th>SKEW_YEO_JONSON</th>\n",
              "      <th>KURT_YEO_JONSON</th>\n",
              "      <th>APSC_YEO_JONSON</th>\n",
              "      <th>MIN_YEO_JONSON</th>\n",
              "      <th>MAX_YEO_JONSON</th>\n",
              "      <th>MEAN_1ST_GRAD_YEO_JONSON</th>\n",
              "      <th>RMSC_YEO_JONSON</th>\n",
              "      <th>STD_1ST_GRAD_YEO_JONSON</th>\n",
              "      <th>RANGE_SQRT</th>\n",
              "      <th>RMSC_SQUARED</th>\n",
              "      <th>MEAN_2ND_GRAD_CUBE</th>\n",
              "      <th>INSC_APSC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005488</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.011382</td>\n",
              "      <td>-0.519100</td>\n",
              "      <td>0.581063</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>0.002382</td>\n",
              "      <td>-0.000477</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>33.000347</td>\n",
              "      <td>0.186582</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.006233</td>\n",
              "      <td>1.228516</td>\n",
              "      <td>44.784180</td>\n",
              "      <td>9.806958</td>\n",
              "      <td>8.117853</td>\n",
              "      <td>1.191406</td>\n",
              "      <td>42.265137</td>\n",
              "      <td>9.571062</td>\n",
              "      <td>8.186006</td>\n",
              "      <td>17</td>\n",
              "      <td>0.005473</td>\n",
              "      <td>0.171077</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.006213</td>\n",
              "      <td>0.011318</td>\n",
              "      <td>3.526371</td>\n",
              "      <td>0.001383</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000477</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000477</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>1.509920</td>\n",
              "      <td>-190.552588</td>\n",
              "      <td>-95.276268</td>\n",
              "      <td>-32.501322</td>\n",
              "      <td>0.004183</td>\n",
              "      <td>0.514110</td>\n",
              "      <td>-0.603458</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.009717</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>0.004874</td>\n",
              "      <td>0.001931</td>\n",
              "      <td>0.106688</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>-1.082030e-10</td>\n",
              "      <td>4803.271908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005027</td>\n",
              "      <td>0.018668</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>0.016721</td>\n",
              "      <td>5.943071</td>\n",
              "      <td>2.244724</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>75.000629</td>\n",
              "      <td>0.382067</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.006055</td>\n",
              "      <td>1.067383</td>\n",
              "      <td>17.068848</td>\n",
              "      <td>2.735987</td>\n",
              "      <td>3.879486</td>\n",
              "      <td>0.873535</td>\n",
              "      <td>17.759766</td>\n",
              "      <td>2.688697</td>\n",
              "      <td>3.890527</td>\n",
              "      <td>4</td>\n",
              "      <td>0.005015</td>\n",
              "      <td>0.323580</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.006036</td>\n",
              "      <td>0.016583</td>\n",
              "      <td>4.330742</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.001943</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>1.673490</td>\n",
              "      <td>-0.197815</td>\n",
              "      <td>-2.854783</td>\n",
              "      <td>-4.359928</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>2.180816</td>\n",
              "      <td>3.085576</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.006785</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>0.001621</td>\n",
              "      <td>0.129309</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>4.920520e-11</td>\n",
              "      <td>10421.930100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.003463</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.003114</td>\n",
              "      <td>1.841308</td>\n",
              "      <td>1.237831</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>-0.000026</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>50.000019</td>\n",
              "      <td>0.054390</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>2.039551</td>\n",
              "      <td>20.222656</td>\n",
              "      <td>4.111731</td>\n",
              "      <td>5.567012</td>\n",
              "      <td>1.783203</td>\n",
              "      <td>20.322754</td>\n",
              "      <td>4.199508</td>\n",
              "      <td>5.566494</td>\n",
              "      <td>10</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.052962</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.003109</td>\n",
              "      <td>3.931826</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>-0.000026</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>-0.000026</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1.595709</td>\n",
              "      <td>-1289.202110</td>\n",
              "      <td>-644.601021</td>\n",
              "      <td>-41.648431</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.917079</td>\n",
              "      <td>1.308226</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.002811</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.055803</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-1.755513e-14</td>\n",
              "      <td>34355.394127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.020365</td>\n",
              "      <td>0.002132</td>\n",
              "      <td>0.018232</td>\n",
              "      <td>5.889044</td>\n",
              "      <td>1.886842</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.002185</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>73.000698</td>\n",
              "      <td>0.424125</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.006502</td>\n",
              "      <td>1.187988</td>\n",
              "      <td>19.096680</td>\n",
              "      <td>3.265896</td>\n",
              "      <td>3.974282</td>\n",
              "      <td>1.287109</td>\n",
              "      <td>17.394043</td>\n",
              "      <td>3.033075</td>\n",
              "      <td>3.970991</td>\n",
              "      <td>7</td>\n",
              "      <td>0.005715</td>\n",
              "      <td>0.353558</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.006481</td>\n",
              "      <td>0.018068</td>\n",
              "      <td>4.304075</td>\n",
              "      <td>0.002130</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.002128</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>1.668475</td>\n",
              "      <td>-13.591159</td>\n",
              "      <td>-6.795579</td>\n",
              "      <td>-3.608363</td>\n",
              "      <td>0.004832</td>\n",
              "      <td>1.521961</td>\n",
              "      <td>2.964263</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.017080</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.135027</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>-3.768462e-13</td>\n",
              "      <td>10030.759444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.011977</td>\n",
              "      <td>0.032645</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.028145</td>\n",
              "      <td>4.503018</td>\n",
              "      <td>2.070755</td>\n",
              "      <td>-0.001407</td>\n",
              "      <td>0.011272</td>\n",
              "      <td>-0.006015</td>\n",
              "      <td>0.010447</td>\n",
              "      <td>5.000520</td>\n",
              "      <td>0.071859</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.015349</td>\n",
              "      <td>4.138672</td>\n",
              "      <td>85.327148</td>\n",
              "      <td>29.969170</td>\n",
              "      <td>31.376367</td>\n",
              "      <td>4.520020</td>\n",
              "      <td>83.107910</td>\n",
              "      <td>29.022323</td>\n",
              "      <td>31.334668</td>\n",
              "      <td>13</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.069395</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.015233</td>\n",
              "      <td>0.027756</td>\n",
              "      <td>1.791846</td>\n",
              "      <td>0.004490</td>\n",
              "      <td>-0.001408</td>\n",
              "      <td>-0.006033</td>\n",
              "      <td>0.004480</td>\n",
              "      <td>-0.001409</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>1.026703</td>\n",
              "      <td>-11.559561</td>\n",
              "      <td>-5.779781</td>\n",
              "      <td>-4.197678</td>\n",
              "      <td>0.007718</td>\n",
              "      <td>2.554131</td>\n",
              "      <td>3.072284</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.002937</td>\n",
              "      <td>0.021636</td>\n",
              "      <td>-0.001400</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>0.006486</td>\n",
              "      <td>0.167764</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>-2.176592e-07</td>\n",
              "      <td>305.010962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       MEAN       MAX       MIN  ...  RMSC_SQUARED  MEAN_2ND_GRAD_CUBE     INSC_APSC\n",
              "0  0.005488  0.012766  0.001384  ...      0.000039       -1.082030e-10   4803.271908\n",
              "1  0.005027  0.018668  0.001947  ...      0.000037        4.920520e-11  10421.930100\n",
              "2  0.001066  0.003463  0.000349  ...      0.000002       -1.755513e-14  34355.394127\n",
              "3  0.005731  0.020365  0.002132  ...      0.000042       -3.768462e-13  10030.759444\n",
              "4  0.011977  0.032645  0.004500  ...      0.000236       -2.176592e-07    305.010962\n",
              "\n",
              "[5 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry_CmUPtNYhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b5b5ba4-c6a3-4314-e2cd-24a3d4c9a1ee"
      },
      "source": [
        "y = df['NasaTLX Label'].values\n",
        "y[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2a4Yifa9j8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c01e4de4-8f36-411d-f407-c274c78ea46f"
      },
      "source": [
        "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n",
        "X[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.11772266e-01, -2.99895377e-01, -3.01948694e-01,\n",
              "        -2.99188712e-01, -9.49891454e-01, -1.33886671e+00,\n",
              "         6.24929122e-02, -2.51592898e-01, -6.37489374e-02,\n",
              "        -2.20167824e-01, -2.15582837e-03, -1.49537465e-01,\n",
              "        -1.48587336e-01, -2.37740741e-01, -1.47074459e-01,\n",
              "        -4.63557736e-02, -1.66696689e-01, -3.29733107e-01,\n",
              "        -1.62309124e-01, -1.18661492e-01, -1.75512823e-01,\n",
              "        -3.27003569e-01,  6.83638760e-01, -2.14520795e-01,\n",
              "        -8.02802870e-02, -1.50409905e-01, -2.42744797e-01,\n",
              "        -3.14882388e-01,  2.91731258e-01, -3.04431793e-01,\n",
              "         6.49073015e-02, -5.78110342e-02, -3.06807581e-01,\n",
              "         6.69805952e-02, -5.14881057e-02, -1.52193178e-01,\n",
              "         3.34147248e-01,  1.49467474e-01,  1.50332905e-01,\n",
              "        -6.52134432e-02, -1.30194490e-01, -9.12056682e-01,\n",
              "        -1.21637337e+00, -2.31831429e-01, -2.55173180e-01,\n",
              "        -2.52782958e-01,  3.23833656e-02, -1.55921685e-01,\n",
              "        -2.08807748e-01, -2.84043371e-01, -1.48587336e-01,\n",
              "         2.96756164e-02, -3.41006032e-01],\n",
              "       [-2.35680446e-01, -2.00632338e-01, -2.16067762e-01,\n",
              "        -1.98419581e-01,  4.28229552e-01,  5.77168271e-01,\n",
              "         7.83189560e-02, -2.17497485e-01,  5.66551109e-02,\n",
              "        -2.34972183e-01,  2.15556230e+00,  2.59680079e-01,\n",
              "        -1.49047108e-01, -2.45002005e-01, -1.60774402e-01,\n",
              "        -9.29191210e-01, -8.39623946e-01, -6.21945012e-01,\n",
              "        -1.89678375e-01, -8.99301585e-01, -8.30821309e-01,\n",
              "        -6.24841855e-01, -1.11368112e+00, -2.39853671e-01,\n",
              "         5.49021844e-01, -1.50882763e-01, -2.50543317e-01,\n",
              "        -1.99907036e-01,  1.38507905e+00, -2.16859165e-01,\n",
              "         8.02632050e-02,  5.80462089e-02, -2.17574824e-01,\n",
              "         8.17830662e-02,  5.90729731e-02, -1.52679005e-01,\n",
              "         1.16045092e+00,  3.37665228e-01,  3.33131934e-01,\n",
              "         3.26160805e-01, -3.20895796e-01,  8.71908672e-01,\n",
              "         6.05363525e-01, -2.60218077e-01, -3.64333079e-01,\n",
              "        -3.63653112e-01,  5.31616674e-02, -3.11587142e-01,\n",
              "        -2.79646399e-01, -4.90655326e-02, -1.49047108e-01,\n",
              "         2.96770259e-02, -1.57825397e-02],\n",
              "       [-4.41317523e-01, -4.56365000e-01, -4.59753629e-01,\n",
              "        -4.55256967e-01, -4.46511490e-01, -5.82468389e-01,\n",
              "         6.56401375e-02, -3.91728721e-01,  6.02330036e-04,\n",
              "        -3.77008775e-01,  8.71183589e-01, -4.26259534e-01,\n",
              "        -1.56428916e-01, -4.40884081e-01, -7.81180747e-02,\n",
              "        -8.28730786e-01, -7.08697734e-01, -5.05599466e-01,\n",
              "        -1.11354390e-01, -8.17655345e-01, -6.86968529e-01,\n",
              "        -5.08634287e-01, -2.84148866e-01, -4.58225223e-01,\n",
              "        -5.67675360e-01, -1.58474815e-01, -4.61440296e-01,\n",
              "        -4.94156184e-01,  8.42849715e-01, -4.65473623e-01,\n",
              "         6.79611960e-02,  4.12229354e-03, -4.71033125e-01,\n",
              "         6.99245833e-02,  7.62566856e-03, -1.60479425e-01,\n",
              "         7.67525506e-01, -9.36732617e-01, -9.36168048e-01,\n",
              "        -1.92426152e-01, -5.30999263e-01, -4.80737396e-01,\n",
              "        -2.72336189e-01, -2.73857442e-01, -5.21264153e-01,\n",
              "        -5.13879712e-01,  3.64952016e-02, -5.38839486e-01,\n",
              "        -5.48094509e-01, -8.12599924e-01, -1.56428916e-01,\n",
              "         2.96765851e-02,  1.36955262e+00],\n",
              "       [-1.99118122e-01, -1.72089626e-01, -1.87771058e-01,\n",
              "        -1.69889503e-01,  4.16707803e-01,  1.64996763e-01,\n",
              "         6.90765260e-02, -2.66454288e-01, -6.00249033e-03,\n",
              "        -2.50952538e-01,  2.05281806e+00,  3.47721385e-01,\n",
              "        -1.47863865e-01, -2.26716538e-01, -1.50520202e-01,\n",
              "        -8.64597288e-01, -7.89193858e-01, -6.15409358e-01,\n",
              "        -1.54068920e-01, -9.10952002e-01, -7.98031092e-01,\n",
              "        -6.19262713e-01, -6.98914992e-01, -2.01117281e-01,\n",
              "         6.72722117e-01, -1.49665844e-01, -2.30907555e-01,\n",
              "        -1.67464495e-01,  1.34883159e+00, -1.88015846e-01,\n",
              "         7.12955861e-02, -2.23307164e-03, -1.88195675e-01,\n",
              "         7.31388833e-02,  1.56086207e-03, -1.51428712e-01,\n",
              "         1.13511639e+00,  3.24423652e-01,  3.25337494e-01,\n",
              "         3.36613141e-01, -5.18584529e-02,  1.66701039e-01,\n",
              "         5.45456164e-01, -2.27864050e-01, -8.54014690e-03,\n",
              "         2.55896903e-02,  4.09824041e-02, -8.55789854e-02,\n",
              "        -2.01154019e-01,  1.03297029e-02, -1.47863865e-01,\n",
              "         2.96765819e-02, -3.84245803e-02],\n",
              "       [ 1.25122954e-01,  3.44594230e-02,  1.73403600e-01,\n",
              "         1.72145317e-02,  1.21124279e-01,  3.76809175e-01,\n",
              "        -9.98445785e-02,  4.17824204e-01, -8.54884017e-01,\n",
              "         5.02993894e-01, -1.44061604e+00, -3.89689526e-01,\n",
              "        -1.07181565e-01,  1.34566596e-01,  1.00354830e-01,\n",
              "         1.24508715e+00,  1.75209266e+00,  1.27381222e+00,\n",
              "         1.24290236e-01,  1.18242089e+00,  1.67655045e+00,\n",
              "         1.27806985e+00,  1.30617259e-01,  1.41221725e-01,\n",
              "        -4.99865974e-01, -1.07829865e-01,  1.55273622e-01,\n",
              "         4.41118555e-02, -2.06593568e+00,  1.79667470e-01,\n",
              "        -9.27259324e-02, -8.21509484e-01,  1.85843359e-01,\n",
              "        -8.50856406e-02, -7.82617262e-01, -1.08449674e-01,\n",
              "        -2.10691474e+00,  3.26432228e-01,  3.27346627e-01,\n",
              "         3.28417290e-01,  2.96031777e-01,  1.27148780e+00,\n",
              "         5.98799877e-01, -4.98102428e-02,  3.78210775e-01,\n",
              "         1.97815412e-01, -1.78544593e-01,  3.43496100e-01,\n",
              "         8.30079707e-01,  3.50381520e-01, -1.07181565e-01,\n",
              "         2.77275771e-02, -6.01377830e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW_kKLSaNcd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PfQ6p3-vvOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6hVR0MIlh2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b6d9fce-3b00-4d4b-8efd-a1e007c590a7"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78788,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmjf7jOqNgt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dd7ec2b-a556-48d3-a426-8058f36da957"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(53, activation='relu', input_dim=53))\n",
        "model.add(Dense(26, activation='relu'))\n",
        "model.add(Dense(13, activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=220) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 53)                2862      \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 26)                1404      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 13)                351       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 7)                 98        \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 3)                 24        \n",
            "=================================================================\n",
            "Total params: 4,739\n",
            "Trainable params: 4,739\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/220\n",
            "78788/78788 [==============================] - 4s 53us/step - loss: 0.4102 - acc: 0.8287\n",
            "Epoch 2/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.1562 - acc: 0.9421\n",
            "Epoch 3/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.1072 - acc: 0.9633\n",
            "Epoch 4/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0897 - acc: 0.9703\n",
            "Epoch 5/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0709 - acc: 0.9759\n",
            "Epoch 6/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0612 - acc: 0.9788\n",
            "Epoch 7/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0577 - acc: 0.9803\n",
            "Epoch 8/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0491 - acc: 0.9831\n",
            "Epoch 9/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0529 - acc: 0.9849\n",
            "Epoch 10/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0519 - acc: 0.9850\n",
            "Epoch 11/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0382 - acc: 0.9875\n",
            "Epoch 12/220\n",
            "78788/78788 [==============================] - 4s 46us/step - loss: 0.0352 - acc: 0.9884\n",
            "Epoch 13/220\n",
            "78788/78788 [==============================] - 4s 48us/step - loss: 0.0432 - acc: 0.9882\n",
            "Epoch 14/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0513 - acc: 0.9869\n",
            "Epoch 15/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0380 - acc: 0.9891\n",
            "Epoch 16/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0308 - acc: 0.9908\n",
            "Epoch 17/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0428 - acc: 0.9899\n",
            "Epoch 18/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0554 - acc: 0.9897\n",
            "Epoch 19/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0324 - acc: 0.9912\n",
            "Epoch 20/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0232 - acc: 0.9924\n",
            "Epoch 21/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0244 - acc: 0.9927\n",
            "Epoch 22/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0226 - acc: 0.9932\n",
            "Epoch 23/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0229 - acc: 0.9928\n",
            "Epoch 24/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0213 - acc: 0.9938\n",
            "Epoch 25/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0219 - acc: 0.9933\n",
            "Epoch 26/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0192 - acc: 0.9944\n",
            "Epoch 27/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0301 - acc: 0.9940\n",
            "Epoch 28/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0187 - acc: 0.9943\n",
            "Epoch 29/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0169 - acc: 0.9952\n",
            "Epoch 30/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0177 - acc: 0.9947\n",
            "Epoch 31/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0160 - acc: 0.9953\n",
            "Epoch 32/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0156 - acc: 0.9954\n",
            "Epoch 33/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0166 - acc: 0.9956\n",
            "Epoch 34/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0172 - acc: 0.9953\n",
            "Epoch 35/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0196 - acc: 0.9950\n",
            "Epoch 36/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0179 - acc: 0.9961\n",
            "Epoch 37/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0172 - acc: 0.9953\n",
            "Epoch 38/220\n",
            "78788/78788 [==============================] - 3s 44us/step - loss: 0.0144 - acc: 0.9959\n",
            "Epoch 39/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0153 - acc: 0.9959\n",
            "Epoch 40/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0192 - acc: 0.9952\n",
            "Epoch 41/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0075 - acc: 0.9978\n",
            "Epoch 42/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0133 - acc: 0.9961\n",
            "Epoch 43/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0175 - acc: 0.9962\n",
            "Epoch 44/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0270 - acc: 0.9952\n",
            "Epoch 45/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0225 - acc: 0.9966\n",
            "Epoch 46/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0170 - acc: 0.9962\n",
            "Epoch 47/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0110 - acc: 0.9971\n",
            "Epoch 48/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0123 - acc: 0.9965\n",
            "Epoch 49/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0163 - acc: 0.9961\n",
            "Epoch 50/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0111 - acc: 0.9976\n",
            "Epoch 51/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0125 - acc: 0.9969\n",
            "Epoch 52/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0113 - acc: 0.9968\n",
            "Epoch 53/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0132 - acc: 0.9969\n",
            "Epoch 54/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0334 - acc: 0.9956\n",
            "Epoch 55/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0134 - acc: 0.9970\n",
            "Epoch 56/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0089 - acc: 0.9975\n",
            "Epoch 57/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0093 - acc: 0.9972\n",
            "Epoch 58/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0151 - acc: 0.9967\n",
            "Epoch 59/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0106 - acc: 0.9973\n",
            "Epoch 60/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0130 - acc: 0.9967\n",
            "Epoch 61/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0099 - acc: 0.9977\n",
            "Epoch 62/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0094 - acc: 0.9975\n",
            "Epoch 63/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0087 - acc: 0.9977\n",
            "Epoch 64/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0092 - acc: 0.9977\n",
            "Epoch 65/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0107 - acc: 0.9972\n",
            "Epoch 66/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0083 - acc: 0.9978\n",
            "Epoch 67/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0137 - acc: 0.9973\n",
            "Epoch 68/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0094 - acc: 0.9974\n",
            "Epoch 69/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0103 - acc: 0.9975\n",
            "Epoch 70/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0090 - acc: 0.9977\n",
            "Epoch 71/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0072 - acc: 0.9981\n",
            "Epoch 72/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0087 - acc: 0.9975\n",
            "Epoch 73/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0080 - acc: 0.9978\n",
            "Epoch 74/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0085 - acc: 0.9979\n",
            "Epoch 75/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0083 - acc: 0.9978\n",
            "Epoch 76/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0075 - acc: 0.9980\n",
            "Epoch 77/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0067 - acc: 0.9985\n",
            "Epoch 78/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0110 - acc: 0.9974\n",
            "Epoch 79/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0079 - acc: 0.9979\n",
            "Epoch 80/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0231 - acc: 0.9970\n",
            "Epoch 81/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0159 - acc: 0.9975\n",
            "Epoch 82/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0118 - acc: 0.9977\n",
            "Epoch 83/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0094 - acc: 0.9976\n",
            "Epoch 84/220\n",
            "78788/78788 [==============================] - 3s 44us/step - loss: 0.0074 - acc: 0.9982\n",
            "Epoch 85/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0092 - acc: 0.9978\n",
            "Epoch 86/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0074 - acc: 0.9980\n",
            "Epoch 87/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0279 - acc: 0.9965\n",
            "Epoch 88/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0326 - acc: 0.9965\n",
            "Epoch 89/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0106 - acc: 0.9986\n",
            "Epoch 90/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0342 - acc: 0.9967\n",
            "Epoch 91/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0357 - acc: 0.9961\n",
            "Epoch 92/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0459 - acc: 0.9956\n",
            "Epoch 93/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0459 - acc: 0.9964\n",
            "Epoch 94/220\n",
            "78788/78788 [==============================] - 3s 44us/step - loss: 0.0486 - acc: 0.9956\n",
            "Epoch 95/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0401 - acc: 0.9962\n",
            "Epoch 96/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0354 - acc: 0.9965\n",
            "Epoch 97/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0313 - acc: 0.9967\n",
            "Epoch 98/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0491 - acc: 0.9956\n",
            "Epoch 99/220\n",
            "78788/78788 [==============================] - 4s 46us/step - loss: 0.0596 - acc: 0.9957\n",
            "Epoch 100/220\n",
            "78788/78788 [==============================] - 4s 48us/step - loss: 0.0548 - acc: 0.9954\n",
            "Epoch 101/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0085 - acc: 0.9985\n",
            "Epoch 102/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0355 - acc: 0.9965\n",
            "Epoch 103/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0096 - acc: 0.9983\n",
            "Epoch 104/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0051 - acc: 0.9987\n",
            "Epoch 105/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0133 - acc: 0.9979\n",
            "Epoch 106/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0248 - acc: 0.9972\n",
            "Epoch 107/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0444 - acc: 0.9963\n",
            "Epoch 108/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0162 - acc: 0.9976\n",
            "Epoch 109/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0333 - acc: 0.9970\n",
            "Epoch 110/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0098 - acc: 0.9984\n",
            "Epoch 111/220\n",
            "78788/78788 [==============================] - 4s 46us/step - loss: 0.0070 - acc: 0.9985\n",
            "Epoch 112/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0238 - acc: 0.9976\n",
            "Epoch 113/220\n",
            "78788/78788 [==============================] - 4s 48us/step - loss: 0.0165 - acc: 0.9977\n",
            "Epoch 114/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 115/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0180 - acc: 0.9971\n",
            "Epoch 116/220\n",
            "78788/78788 [==============================] - 3s 44us/step - loss: 0.0061 - acc: 0.9992\n",
            "Epoch 117/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0104 - acc: 0.9985\n",
            "Epoch 118/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0063 - acc: 0.9984\n",
            "Epoch 119/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0216 - acc: 0.9977\n",
            "Epoch 120/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 121/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0061 - acc: 0.9986\n",
            "Epoch 122/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0062 - acc: 0.9985\n",
            "Epoch 123/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0059 - acc: 0.9986\n",
            "Epoch 124/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0099 - acc: 0.9984\n",
            "Epoch 125/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0320 - acc: 0.9972\n",
            "Epoch 126/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0374 - acc: 0.9967\n",
            "Epoch 127/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0334 - acc: 0.9968\n",
            "Epoch 128/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0188 - acc: 0.9982\n",
            "Epoch 129/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0196 - acc: 0.9977\n",
            "Epoch 130/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0050 - acc: 0.9988\n",
            "Epoch 131/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0050 - acc: 0.9988\n",
            "Epoch 132/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0055 - acc: 0.9986\n",
            "Epoch 133/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 134/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 135/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0038 - acc: 0.9990\n",
            "Epoch 136/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0062 - acc: 0.9984\n",
            "Epoch 137/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0103 - acc: 0.9985\n",
            "Epoch 138/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0189 - acc: 0.9982\n",
            "Epoch 139/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0044 - acc: 0.9989\n",
            "Epoch 140/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0227 - acc: 0.9973\n",
            "Epoch 141/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0282 - acc: 0.9977\n",
            "Epoch 142/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0340 - acc: 0.9970\n",
            "Epoch 143/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0060 - acc: 0.9990\n",
            "Epoch 144/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0169 - acc: 0.9980\n",
            "Epoch 145/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0251 - acc: 0.9974\n",
            "Epoch 146/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 147/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0068 - acc: 0.9988\n",
            "Epoch 148/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0045 - acc: 0.9991\n",
            "Epoch 149/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0032 - acc: 0.9994\n",
            "Epoch 150/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0064 - acc: 0.9985\n",
            "Epoch 151/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0039 - acc: 0.9990\n",
            "Epoch 152/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0068 - acc: 0.9990\n",
            "Epoch 153/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0051 - acc: 0.9990\n",
            "Epoch 154/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0055 - acc: 0.9993\n",
            "Epoch 155/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0066 - acc: 0.9989\n",
            "Epoch 156/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0045 - acc: 0.9991\n",
            "Epoch 157/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0068 - acc: 0.9987\n",
            "Epoch 158/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0084 - acc: 0.9990\n",
            "Epoch 159/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0048 - acc: 0.9989\n",
            "Epoch 160/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0046 - acc: 0.9989\n",
            "Epoch 161/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0059 - acc: 0.9990\n",
            "Epoch 162/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0038 - acc: 0.9992\n",
            "Epoch 163/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0054 - acc: 0.9989\n",
            "Epoch 164/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0068 - acc: 0.9989\n",
            "Epoch 165/220\n",
            "78788/78788 [==============================] - 4s 49us/step - loss: 0.0071 - acc: 0.9990\n",
            "Epoch 166/220\n",
            "78788/78788 [==============================] - 4s 51us/step - loss: 0.0087 - acc: 0.9986\n",
            "Epoch 167/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0078 - acc: 0.9990\n",
            "Epoch 168/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0035 - acc: 0.9991\n",
            "Epoch 169/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0046 - acc: 0.9991\n",
            "Epoch 170/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0029 - acc: 0.9994\n",
            "Epoch 171/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0072 - acc: 0.9986\n",
            "Epoch 172/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 173/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0043 - acc: 0.9990\n",
            "Epoch 174/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0041 - acc: 0.9991\n",
            "Epoch 175/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0043 - acc: 0.9991\n",
            "Epoch 176/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0035 - acc: 0.9991\n",
            "Epoch 177/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0051 - acc: 0.9987\n",
            "Epoch 178/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0290 - acc: 0.9972\n",
            "Epoch 179/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0307 - acc: 0.9977\n",
            "Epoch 180/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0358 - acc: 0.9973\n",
            "Epoch 181/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0208 - acc: 0.9980\n",
            "Epoch 182/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0029 - acc: 0.9995\n",
            "Epoch 183/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0036 - acc: 0.9992\n",
            "Epoch 184/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0060 - acc: 0.9987\n",
            "Epoch 185/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0042 - acc: 0.9990\n",
            "Epoch 186/220\n",
            "78788/78788 [==============================] - 4s 48us/step - loss: 0.0022 - acc: 0.9996\n",
            "Epoch 187/220\n",
            "78788/78788 [==============================] - 4s 46us/step - loss: 0.0051 - acc: 0.9988\n",
            "Epoch 188/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0054 - acc: 0.9989\n",
            "Epoch 189/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0031 - acc: 0.9994\n",
            "Epoch 190/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0050 - acc: 0.9990\n",
            "Epoch 191/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0020 - acc: 0.9996\n",
            "Epoch 192/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0038 - acc: 0.9994\n",
            "Epoch 193/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0054 - acc: 0.9989\n",
            "Epoch 194/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0211 - acc: 0.9981\n",
            "Epoch 195/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0023 - acc: 0.9995\n",
            "Epoch 196/220\n",
            "78788/78788 [==============================] - 4s 44us/step - loss: 0.0054 - acc: 0.9989\n",
            "Epoch 197/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0089 - acc: 0.9985\n",
            "Epoch 198/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0020 - acc: 0.9996\n",
            "Epoch 199/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0041 - acc: 0.9989\n",
            "Epoch 200/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0026 - acc: 0.9994\n",
            "Epoch 201/220\n",
            "78788/78788 [==============================] - 4s 47us/step - loss: 0.0056 - acc: 0.9988\n",
            "Epoch 202/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0018 - acc: 0.9997\n",
            "Epoch 203/220\n",
            "78788/78788 [==============================] - 3s 44us/step - loss: 0.0039 - acc: 0.9992\n",
            "Epoch 204/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0021 - acc: 0.9997\n",
            "Epoch 205/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0044 - acc: 0.9991\n",
            "Epoch 206/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0032 - acc: 0.9993\n",
            "Epoch 207/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0046 - acc: 0.9991\n",
            "Epoch 208/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 209/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0154 - acc: 0.9983\n",
            "Epoch 210/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0228 - acc: 0.9977\n",
            "Epoch 211/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0022 - acc: 0.9996\n",
            "Epoch 212/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0052 - acc: 0.9990\n",
            "Epoch 213/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0025 - acc: 0.9995\n",
            "Epoch 214/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0055 - acc: 0.9991\n",
            "Epoch 215/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0030 - acc: 0.9993\n",
            "Epoch 216/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0029 - acc: 0.9993\n",
            "Epoch 217/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 218/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0029 - acc: 0.9994\n",
            "Epoch 219/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0071 - acc: 0.9993\n",
            "Epoch 220/220\n",
            "78788/78788 [==============================] - 4s 45us/step - loss: 0.0082 - acc: 0.9987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7eff972c1cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uha9x63Njut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "efdf6e58-422a-48e1-a5ca-a0dd5a5f75fd"
      },
      "source": [
        "scores = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
        "scores = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 99.93%\n",
            "\n",
            "Testing Accuracy: 99.89%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIP1-FziNprg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "e7f0ecf5-a92f-4b4e-d1a6-427be8b0ee34"
      },
      "source": [
        "y_test_pred = model.predict_classes(X_test)\n",
        "c_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "ax = sns.heatmap(c_matrix, annot=True, xticklabels=['Low','Medium', 'High'], yticklabels=['Low','Medium', 'High'], cbar=False, cmap='Blues')\n",
        "ax.set_xlabel(\"Prediction\")\n",
        "ax.set_ylabel(\"Actual\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEGCAYAAABmXi5tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAd1UlEQVR4nO3dd5hU5fnG8e/DLsvCwgKLFBEBUcQA\n0gTEqAhR0CBqVGwItkQkvygWUCQWRBMrmkSJDQULWGIhGOzSkSIoKIgtIhKQorCUXdqW5/fHDOuC\nW2ZhzwzsuT/XtdfOKXPeZ5iLm5f3nPMec3dERKTiq5ToAkREJD4U+CIiIaHAFxEJCQW+iEhIKPBF\nREIiOdEFFKdq+6t1+dABKnP+qESXIBJaqclYcdvUwxcRCQkFvohISCjwRURCQoEvIhISCnwRkZBQ\n4IuIhIQCX0QkJBT4IiIhocAXEQkJBb6ISEgo8EVEQkKBLyISEgp8EZGQUOCLiISEAl9EJCQU+CIi\nIaHAFxEJCQW+iEhIKPBFREJCgS8iEhIKfBGRkFDgi4iEhAJfRCQkFPgiIiGhwBcRCQkFvohISCjw\nRURCQoEvIhISCnwRkZBQ4IuIhIQCX0QkJJITXcCBomb1qjw2vC8tDz8Ydxg4YjzzPvuuYHutGlV5\n4o5+HNboIHbszOGqO8az9NvV+9RmSuVknr6rP+1/1ZgNm7LpN3QMK1ZvoGOrJoy67SIAzOCvj7/F\nG1M/26e2JHZ5eXlcdP651Ktfn1GPPpHociRGa1av5pZhN7Fh/Xowo89553Nx/0sTXVZcKfBjNPKm\nPrw3eyl9b3yayslJVEtN2W37Tb8/lU+/WskFg0dzZNP6/P3m8+k18JGYjt344AxG39mfU6/8x27r\nL/vdcWRu2Ubrs0Zw3qnH8Ndrz6L/zWP5/NsfOP7i+8nLy6fBQenMe3kYb85YQl5efrl9Xine+Oef\no1mzw8nKzkp0KVIGSclJDLnpZn7VshXZ2VlceN65dDnueA4/4ohElxY3GtKJQXr1VE7ocDjPTJgD\nQE5uHpuytu22z1HNGjB9/tcAfL18LU0aZlAvowYAF/bqxMznhzD3pZt55JYLqVTJYmq3d7c2jP/P\nPABe/2Ah3Tq3AGDb9pyCcK+SUhl33/cPKTFZu2YNM2dM4+xz+yS6FCmjunXr8auWrQBIS6tOs2bN\nWLdubYKriq9AA9/MUoM8frw0bViHnzKzeHJEP+a8OJRHb+/7ix7+4q9XcdZv2gLQsVUTGh+cwSH1\na9HisPr06dmB7pc/RJcL7yUvP58Le3WKqd2G9Wqyck0mAHl5+WzO2kadWmkAdGrdhI9fvYUFr/yZ\nQX99Sb37OLn/3ru5fvCNVKqkvtKBbNWqlXz5xRcc3aZtokuJq6CHdJaY2VpgZvRnlrtvCrjNcpec\nnES7ow7lhvteYf6S7xl547kMuaIHdz76ZsE+I8e+z8gb+zD3pZv5/Jsf+PSrleTl5dO9cws6tGzM\nrHE3AVC1SmV+3BAZCnj5wStpckgdUioncWiDDOa+dDMA/3xhGs+/MbfEmuYv+Z5j+vyVFofV56k7\n+/Puh0vZsTM3mD8AAWD6tKlkZGTQslVr5n80L9HlyF7amp3N4OsGcePNf6Z69eqJLieuAg18dz/C\nzBoDJwKnA/80s43u3q6o/c1sADAAILlRN5IPahVkeTFbtTaTVes2Mn/J9wBM+GARgy/vsds+W7K3\nc9Ud4wqWv3xzBN+tWs/xHY5g3H/mcfsjb/ziuBcMHg0UP4b/w7pNNGpQm1XrNpKUVIn06lVZvzF7\nt32++m4tWVt30OqIhnyydEW5fF4p2qKFnzBt2hRmzZzBjh07yM7OYtjQIdxz38hElyYxysnJ4Ybr\nBtHr9DM4pUfPRJcTd0EP6TQCjicS+O2Bz4GXi9vf3Z90947u3nF/CXuAteu3sHJNJs2b1AOgW+cW\nfLlszW771KxelcrJSQBcfvavmfXJf9mSvZ2pH33F2ae0o27tSE+idno1Gh9cO6Z235y+mIvPOBaA\nc05pX3COoEnDOiQlRb66xgfXpsVhDfj+h/X7/kGlRNdeP5j3p8zg7fencN/Ih+h0bBeF/QHE3bnj\n9lto1qwZl1x2eaLLSYigh3RWAPOBu919YMBtBeqG+15h7N2XkZKcxPJVPzFg+Dj+0OcEAJ56dRZH\nNWvA6Dv74+588e1qBo4YD8CXy9Yw4p+T+M9jV1PJjJzcPK6/91+sWJ1ZapvP/Hs2Y/5yCUsmDidz\nczb9bx4LwK/bN2PI5T3Jyc0jP9+59u6Xf9HzF5HdLfzkYya9MZHmRx7J+eecBcA1193AiV1PSnBl\n8WNBXuFhZm2BE4CuQGPgG2C6uz9d2nurtr9al54coDLnj0p0CSKhlZpMsZcBBj2G/6mZfQt8S2RY\npx9wElBq4IuISPkKNPDNbAFQBZhN5Cqdru7+fZBtiohI0YIew/+tu/8YcBsiIhKDoO8e2WlmD5nZ\ngujPg2ZWM+A2RUSkCEEH/hhgC3B+9GczMDbgNkVEpAhBD+kc7u7nFloeYWaLAm5TRESKEHQPf5uZ\nnbBrwcyOB7aVsL+IiAQk6B7+QOC5QuP2mUC4JqAWEdlPBH4dPtDWzNKjy5vN7DpAT+sQEYmzuMzx\n6u6b3X1zdPGGeLQpIiK7S8Sk3rE9/UNERMpVIgJfc+SIiCRAIGP4ZraFooPdgKpBtCkiIiULJPDd\nvUYQxxURkb2nB3OKiISEAl9EJCQU+CIiIaHAFxEJCQW+iEhIKPBFREJCgS8iEhIKfBGRkFDgi4iE\nhAJfRCQkFPgiIiGhwBcRCQkFvohISCjwRURCQoEvIhISCnwRkZBQ4IuIhIS575+PmN2eq2ffHqhq\nd7k+0SXIPsic+7dElyD7IDUZK26bevgiIiGhwBcRCQkFvohISCjwRURCQoEvIhISCnwRkZBQ4IuI\nhIQCX0QkJBT4IiIhocAXEQkJBb6ISEgo8EVEQkKBLyISEgp8EZGQUOCLiISEAl9EJCQU+CIiIaHA\nFxEJCQW+iEhIKPBFREJCgS8iEhIKfBGRkFDgi4iEhAJfRCQkkovbYGaPAF7cdncfFEhFIiISiGID\nH1gQtypERCRwxQa+uz8bz0JERCRYJfXwATCzusBQoCWQumu9u/8mwLpERKScxXLSdjzwBXAYMAJY\nDswPsCYREQlALIFfx92fBnLcfbq7XwGody8icoApdUgHyIn+Xm1mpwM/ABnBlSQiIkGIJfD/YmY1\ngcHAI0A6cH2gVYmISLkrNfDdfVL05Sage7DlVHzLv1vGTYN//vdy5cr/8X9XD6LfJZclrqgQqFk9\nlcduu5CWhzfAHQbe+SLzFn9fsD09LZUxd/Xj0Aa1SE5K4u/jpvL8fz7apzZrp1fj+XsuocnBGXy/\negP9bn6WjVu20fuk1tw+8Lfk5zu5efnc9OAEZn/63b5+RIlRXl4eF51/LvXq12fUo08kupy4Mvdi\n762K7GA2liJuwIqO5Qdme27xN31VFHl5efTo3pVxL/2Lhg0PSXQ55aZ2l/3vP4Cj7+jLhwu/5ZmJ\n86icnES11MpsytpesP3Gy0+hZvVUbn1kEgfVSuPT14bR9NTh5OTmlXrsE485nP69OzNgxIu7rf/r\noDPI3LSVkc9OZsilJ1MrvSq3PjKJtKopZG/bCUDrIw5m3L2X0q7PveX7gfdB5ty/JbqEQD33zFiW\nfr6ErOysChn4qclYcdtiOWk7CXgz+jOZyJBOVvmUFm7z5s7h0EMPrVBhvz9KT0vlhPbNeGbiPABy\ncvN2C3sAd6d6tSoApFWrQubmreTm5QNwff/uzHr2ej568UZuHXBazO32Pqk14yZFLmgbN2k+Z3Q7\nGqAg7AHSqqZQSp9LytHaNWuYOWMaZ5/bJ9GlJEQsQzqvFV42sxeBWbE2YGa1gUMLt+Xun5Shxgrr\nnbff5LRevRNdRoXX9JAMftqYxZPDL+LoIxuy8IuVDBk5ga3bfw7ex/81i1cf+j3L3hlBjWpV6D/s\nOdydk49tweGH1uWES/+GmfHqQ7/n+PbN+HDhslLbrZdRgzXrNwOwZv1m6mXUKNh2ZrejufPq06lb\nuzrnXDe6/D+0FOn+e+/m+sE3kp2dnehSEmJvJk9rDtSLZUczuwv4DHgYeDD6M7KE/QeY2QIzW/D0\n6Cf3orQDR87OnUyfOoWep8beY5S9k5yURLsWjRj96occd/GDbN22kyGXnbzbPj2OO4rPvv6BZqcN\n59i+I/nbTedQI60Kp3RpwSldWjB3/BDmjBtMi6b1OKJxXQBmPHMdc8cP4bFbL+D0rq2YO34Ic8cP\n4ZQuLYqso/Dw6RvTFtOuz72cP2QMtw/sFdyHlwLTp00lIyODlq1aJ7qUhInlTtst7D6Gv4bInbex\nOB843N13lron4O5PAk9CxR/DnzVrBke1bEWdgw5KdCkV3qp1G1m1bhPzP18BwITJnzJ4j8Dvf0Zn\nHnxmMgDLVv7E8h820KJpfczggWc+4OnX5/ziuF0v+ztQ/Bj+ug1baFAnnTXrN9OgTjo/Zv5yJPTD\nhcs47JA61KmZxvpN4ex1xsuihZ8wbdoUZs2cwY4dO8jOzmLY0CHcc1+xfdAKp9QevrvXcPf0Qj9H\n7jnMU4IlQK19K7FievutN/ltr9MTXUYorF2/hZVrN9K8SaRn3q1zc75ctma3ff63JpNunZsDUC+j\nOkc2qct3K9fz/pyvuPTMY0mrmgJAw7o1qVu7ekztvjl9Cf16dwKgX+9OTJq+BIBmjX7+R75di0ZU\nSUlS2MfBtdcP5v0pM3j7/SncN/IhOh3bJVRhD7H18Ce7+8mlrSvGPcBCM1sC7Ni10t3PLHOlFcjW\nrVuZO3s2tw2/M9GlhMYND7zG2Lv6k1I5ieWr1jNgxIv84dxfA/DUa7O596n3ePKOvsx/6UbMjFse\nmcT6TdlMnvcVRx1Wn2ljrwUge+tOLr9tXJG99T2NfHYy4+65lEvPOpYVqzPpNywyH+HZJ7ehb69O\n5OTmsX1HDv2HPRfcBxcppNjLMs0sFagGTAW6QcGlPunAO+5+VKkHN/sceAJYDOTvWu/u00t7b0Uf\n0qnI9sfLMiV2Ff2yzIqupMsyS+rhXwVcBzQEPubnwN8MjIqx7a3u/nCM+4qISIBKmg//H8A/zOwa\nd39kL48/08zuAd5g9yEdXZYpIhJnscylk29mtdx9IxRcV3+Ruz8aw3vbR393KbTO0WybIiJxF0vg\nX+nu/9y14O6ZZnYlUGrgu7vm3hER2U/EEvhJZmYePbtrZklASiwHN7Pbi1rv7ro8RUQkzmIJ/HeA\nl81s1yxDVwFvx3j8whcXpwK9iTw9S0RE4iyWwB8KDAAGRpc/AxrEcnB3f7DwspmNBN4tS4EiIlI+\nYrnTNh+YR+RZtp2JnHDd2156NaDRXr5XRET2QbE9fDM7Ergo+vMT8DKU7USsmS3m53l4koC6gMbv\nRUQSoKQhnS+BmUBvd/8vgJmV9RbKwnP/5gJr3T23jMcQEZFyUNKQzjnAamCqmY02s5Oh+Ft2CzOz\n9OjLLYV+tgHpZqYHoIuIJEBJd9r+G/i3maUBZxGZZqGemT0GTHD390o47gtEevcfExnSKfwPhQPN\n9rVwEREpm1ieeJVNJMBfiN5lex6RK3eKDXx37x39fVg51SkiIvsolssyC7h7JpEHlJT4OCoz61DK\ncTSXjohInJUp8Mtg1/X3qUBH4FMiwzptgAXAcQG1KyIixdibZ9qWyt27Ry/fXA10cPeO7n4MkcnU\nVgXRpoiIlCyQwC+khbsv3rXg7kuAXwXcpoiIFCGoIZ1dPjOzp4Bx0eWLiUzNICIicRZ04F8O/BG4\nNro8A3gs4DZFRKQIgQa+u283s8eBt9z9qyDbEhGRkgU6hm9mZwKLiEyxjJm1M7M3gmxTRESKFvRJ\n2+FEZtjcCODuiwDdjCUikgBBB36Ou2/aY50XuaeIiAQq6JO2n5tZXyKPSWwODAJmB9ymiIgUIege\n/jVAK2AH8CKwmcgkbCIiEmdBX6WzFbgl+iMiIgkUSOCXdiWOu58ZRLsiIlK8oHr4xwH/IzKMM48Y\nH5wiIiLBCSrwGwA9iDwPty/wJvCiu38eUHsiIlKKoGbLzHP3d9z9UqAL8F9gmpldHUR7IiJSusBO\n2ppZFeB0Ir38psDDwISg2hMRkZKZe/nfB2VmzwGtgbeAl6LTIpfJ9lzdoCWSCBkXjEl0CbIPtr52\nRbHnTIPq4fcDsonMkjnIrKB9A9zd0wNqV0REihFI4Lt70Dd0iYhIGSmYRURCQoEvIhISCnwRkZBQ\n4IuIhIQCX0QkJBT4IiIhocAXEQkJBb6ISEgo8EVEQkKBLyISEgp8EZGQUOCLiISEAl9EJCQU+CIi\nIaHAFxEJCQW+iEhIKPBFREJCgS8iEhIKfBGRkFDgi4iEhAJfRCQkFPgiIiGhwBcRCQkFvohISCjw\nRURCQoEvIhISCnwRkZBQ4IuIhIQCX0QkJJITXUDY3H7rMGZMn0ZGRh1enzgp0eVIGezYsYPLL7mY\nnJ07yc3Lo0fPU/m/qwcluqxQqlkthUf/73haNq6NOwz850w++vrHvT7exd2OYGiftgDc9+qnjJ/2\nXwAm3tqT+rWrkZxkzF66luuemkN+vpfLZ0gEBX6cnfW7c7iobz9uGTY00aVIGaWkpPDUmGeplpZG\nTk4Ol/XvywkndqVN23aJLi10HrjiWN5fuIqLR06lcnIlqqXEFmXvjPgtA0bNZMWPWQXraldP4c/n\nt+eEmybiDh8+cBZvzl/Bxuyd9HtwKlu25QDwwo2/4ZzjmvLqh98F8pniQUM6cXZMx06k16yZ6DJk\nL5gZ1dLSAMjNzSU3NxfMElxV+KRXq8wJLRvwzOSvAcjJzWfT1p0cVr8GE2/tyYf3n8n7d/XiyENi\n+3t2SrtGTPl0FZlZO9mYvZMpn66iR/tGAAVhn5xkpCRXwg/czj2gHr5ImeTl5XHReeewYsUKLrio\nL23atE10SaHTtF4Nftq8nSeuPpE2TTJYuOwnhoyZx6iBxzPoydl8u3oznZrX5e9XHkevO94p9XgN\nM6qx8qfsguVV67NpmFGtYHnibT3peERd3lu4kglzlwfxkeIm8MA3s0OAJoXbcvcZQbcrEoSkpCT+\n9fpENm/ezPWD/sQ333xN8+ZHJrqsUElOMto1q8Pgp+cy/5sfeeCKYxl+UQe6tKjH+MHdC/ZLqZwE\nQP/uzfnT6S0BaNYgnQm39CAnN5/l67K48P7JpbZ31l3vUaVyEmOvO4lurQ9mymc/BPPB4iDQwDez\n+4ALgKVAXnS1A0UGvpkNAAYAjHr0CX5/5YAgyxPZa+np6XTqfCyzZ81U4MfZqvVbWbU+m/nfRE7S\nTpiznNsu7MCmrTvpMmTiL/Z/fuo3PD/1G6DoMfwfNmyla6sGBcuH1EljxudrdjvGjpw8Jn20gt6d\nGx/QgR/0GP7vgBbu3svdz4j+nFnczu7+pLt3dPeOCnvZ32zYsIHNmzcDsH37dubOmU3Tw5oluKrw\nWbtxGyt/yqZ5w3QAuh/dkE++/Ynla7dw9nFNC/Y7uklGTMf7YNFKTm57CLXSUqiVlsLJbQ/hg0Ur\nSUtNpkGtqgAkVTJOO6YRX6/aVO6fJ56CHtJZBlQGdgTczgFj6JAbWDD/IzZuzKTHb7ryxz9dwznn\nnpfosiQGP/24jlv/fDP5+Xnk5zs9Tz2Nk7p1L/2NUu4GPz2Xsdd2o3LlSixfu4WrRs2kZloKDw/4\nNUP7tKNykvHqh9+x+PsNpR4rM2sn9766iJn3Rfqi97yyiMysndSrmcorw04hpXISlcyYsWQ1o9/9\nMuiPFijzAE47m9kjRIZuDgHaApMpFPruXurFy9tzOcDPh4scmDIuGJPoEmQfbH3timIvHQuqh78g\n+vtj4I2A2hARkTIIJPDd/dkgjisiInsv6Kt0FsMvhmY2EfkfwF/cfX2Q7YuIyM+CPmn7NpHLMV+I\nLl8IVAPWAM8AZwTcvoiIRAUd+Ke4e4dCy4vN7BN372Bm/QJuW0RECgn6OvwkM+u8a8HMOgFJ0cXc\ngNsWEZFCgu7h/wEYY2bVAQM2A38wszTgnoDbFhGRQgINfHefDxxtZjWjy4VvU/tXkG2LiMjuAgl8\nM+vn7uPM7IY91gPg7g8F0a6IiBQvqB5+WvR3jYCOLyIiZRTUjVdPRH+PCOL4IiJSdkEN6Txc0vZY\n5tIREZHyFdSQzseFXo8AhgfUjoiIxCjwuXTM7DrNrSMiknjxeIi5pjkWEdkPxCPwRURkPxDUSdst\n/Nyzr2Zmm3dtAtzd04NoV0REihfUGL6uvxcR2c9oSEdEJCQU+CIiIaHAFxEJCQW+iEhIKPBFREJC\ngS8iEhIKfBGRkFDgi4iEhAJfRCQkFPgiIiGhwBcRCQkFvohISCjwRURCQoEvIhIS5q4HUiWCmQ1w\n9ycTXYfsHX1/B64wf3fq4SfOgEQXIPtE39+BK7TfnQJfRCQkFPgiIiGhwE+cUI4hViD6/g5cof3u\ndNJWRCQk1MMXEQkJBb6ISEgo8ANgZlmJrkF+yczczMYVWk42sx/NbFIZjzPNzDpGX79lZrXKu1Yp\nuz3/3pnZZWY2Kvp6oJldUsr7C/avqJITXYBIHGUDrc2sqrtvA3oAq/blgO7eq1wqk0C5++OJrmF/\noB5+nJhZOzOba2afmdkEM6ttZvXM7OPo9rbRHmjj6PK3ZlYtsVVXSG8Bp0dfXwS8uGuDmaWZ2Rgz\n+8jMFprZWdH1Vc3sJTP7wswmAFULvWe5mR1kZk3NbEmh9UPM7I7o62lm9jczWxA9Ricze93MvjGz\nv8ThM4eemd1hZkOirztF/x4uMrMHCn9vQEMzeyf63dyfoHIDo8CPn+eAoe7eBlgMDHf3dUCqmaUD\nJwILgBPNrAmwzt23Jq7cCusl4EIzSwXaAPMKbbsFmOLunYHuwANmlgb8Edjq7r8ChgPH7EW7O929\nI/A4MBH4E9AauMzM6uz1p5HCqkZDfJGZLQLuLGa/scBV7t4OyNtjWzvgAuBo4AIzOzS4cuNPQzpx\nYGY1gVruPj266lnglejr2cDxQFfgbuA0wICZ8a4zDNz9MzNrSqR3/9Yem3sCZ+7qCQKpQGMi383D\nhd7/2V40/Ub092Lgc3dfDWBmy4BDgfV7cUzZ3bZoiAORMXmgY+Edoudbarj7nOiqF4DehXaZ7O6b\novsuBZoA/wuy6HhS4CfeDCK9+yZEen5DAQfeTGRRFdwbwEigG1C4d23Aue7+VeGdzSyWY+ay+/+Y\nU/fYviP6O7/Q613L+nu4/yj83eRRwb4bDenEQbTHkGlmJ0ZX9Qd29fZnAv2Ab9w9H9gA9AJmxb3Q\n8BgDjHD3xXusfxe4xqIJb2bto+tnAH2j61oTGQra01qgnpnVMbMq7N5rlP2Eu28EtpjZsdFVFyay\nnnirUP967UeqmdnKQssPAZcCj0dPxC4DLgdw9+XRgJkR3XcW0MjdM+NZcJi4+0qiQzR7uAv4O/CZ\nmVUCviMS3I8BY83sC+AL4OMijpljZncCHxG58ufLgMqXffd7YLSZ5RPpeG1KcD1xo6kVRCRUzKy6\nu2dFX98MHOzu1ya4rLhQD19EwuZ0MxtGJP++By5LbDnxox6+iEhI6KStiEhIKPBFREJCgS8iEhIK\nfKmQzCwveov9EjN7ZV/mJTKzZ8ysT/T1U2bWsoR9u5nZrwstlzpLo0i8KPClotrm7u3cvTWwExhY\neKOZ7dUVau7+B3dfWsIu3YCCwHf3x939ub1pS6S8KfAlDGYCR0R73zPN7A1gqZklRWdLnB+dPfEq\nAIsYZWZfmdkHQL1dB9pjLvzTzOwTM/vUzCZH5+gZCFwf/d/FiXvM0viLGVMLHfO+6CydXxe6I1uk\nXOk6fKnQoj353wLvRFd1AFq7+3dmNgDY5O6dotMhfGhm7wHtgRZAS6A+sJTIdAyFj1sXGA10jR4r\nw903mNnjQJa7j4zud3Khtz0HXOPu06N35Q4HrotuS3b3zmbWK7r+lPL+sxBR4EtFVTU6RS5EevhP\nExlq+cjdv4uu7wm02TU+D9QEmhOZHfNFd88DfjCzKUUcvwswY9ex3H1DScWUMmMqwOvR3x8DTWP7\niCJlo8CXimq3qXKhYNbL7MKriPS4391jv0Q8xWrXLI0VboZG2X9oDF/C7F3gj2ZWGcDMjow+8GQG\nkYdfJJnZwUQehrKnuUBXMzss+t6M6PotQI09dy5lxlSRuFBPQsLsKSLDJ59EZyz9EfgdMAH4DZGx\n+xXAnD3f6O4/Rs8BvB6dWXMdkWfk/gd41SKPR7xmj7cVOWOqSLxoLh0RkZDQkI6ISEgo8EVEQkKB\nLyISEgp8EZGQUOCLiISEAl9EJCQU+CIiIfH/WEyzItkkNUwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}